---
title: "R Notebook"
output: pdf_document
---

```{r setup}
library(tidyverse)
library(caret)
library(tidymodels)

train <- read.csv("train.csv")
test <- read.csv("test.csv")
test <- test %>% mutate(loss = 0)
```

```{r}
num_vars <- train %>% select(cont1:cont14, loss)

library(corrplot)
corrplot(cor(num_vars), method = "color", type = "upper", diag = FALSE, addCoef.col = "black", number.digits = 2)

# From the correlation plot, almost none of the continuous variables have a strong correlation with the loss,
# but it looks like there is a good amount of correlation among the continuous variables. Multicolinearity may be a problem with these
```

```{r}
ggplot(train, aes(x = loss)) + geom_histogram(fill = "firebrick4")
train %>%
    filter(loss > 20000) %>%
    summarize(n(), n()/nrow(train))

library(e1071)
skewness(train$loss) # Positive means right skewed, negative means left skewed
# There is a large right skew to the distribution of loss. About .2% of the observations have a loss greater than 20000
box_trans <- BoxCoxTrans(train$loss)
box_trans
# With lambda estimated as 0, the best transformation is the natural log of loss

# Now it is approximately normal with a few outliers
ggplot(train, aes(x = log(loss))) + geom_histogram(fill = "firebrick4")
```


```{r}

set.seed(041796)
# I am going to take part of the training data to save computation time
samp <- sample(1:nrow(train), size = (.1*nrow(train)), replace = FALSE)
small_train <- train[samp, ]
ggplot(small_train, aes(x = log(loss))) + geom_histogram(fill = "firebrick4")
ggplot(small_train, aes(x = loss)) + geom_histogram(fill = "firebrick4")
```

```{r}
# Create an X matrix 
dat <- rbind(train, test) %>% select(-cat109, -cat110, -cat116)
Xmat <- model.matrix(log(loss)~., data = dat)
y <- log(dat$loss[1:nrow(train)])
train.mat <- Xmat[1:nrow(train),]
test.mat <- Xmat[(nrow(train) + 1):nrow(Xmat),] 
samp <- sample(1:nrow(train), size = (.1*nrow(train)), replace = FALSE)
small_train_mat <- train.mat[samp,]
y_small <- y[samp]
```

```{r}
myControl <- trainControl(method = "boot",
                          number = 3, allowParallel = TRUE)

total_prop <- function(data, col){
  data %>% dplyr::group_by(col) %>% 
    dplyr::summarize(prop = n()/length(col))
}

total_prop(train, cont1) # Error: Must group by variables found in `.data`. * Column `col` is not found.
train %>% group_by(cat108) %>%
summarize(prop = n()/nrow(train))

# Need to make an X matrix
removed_vars <- small_train %>% select(-id, -cat89, -cat92, -cat96, -cat99, -cat103, -cat106, -cat109, -cat110, -cat111, -cat113, -cat115, -cat116, -cat114, -cat112)
grid <- expand.grid("alpha" = c(0,.5,1), "lambda" = c(.1, .05,.5, .75, .0001))
glmnet.mod <- train(x = small_train_mat, 
                    y = y_small,
                    method = "glmnet",
                    trControl = myControl,
                    tuneGrid = grid)
glmnet.mod
beepr::beep(sound = 8)
preds <- predict(glmnet.mod, test.mat) %>% exp()
glmnet.preds <- data.frame(id = test$id, loss = preds)
write_csv(glmnet.preds, "glmnet-preds-mat.csv")
```

```{r}
rf.grid <- expand.grid("mtry" = c(20), "splitrule" = "extratrees", "min.node.size" = 1)
# 20 came out as the best
rf.model <- train(x = small_train_mat, 
                  y = y_small,
                  method = "ranger",
                  trControl = myControl,
                  tuneGrid = rf.grid, 
                  importance = "permutation")
rf.model
beepr::beep(sound = 2)
rf.preds <- predict(rf.model, test.mat) %>% exp()
rf.preds.frame <- data.frame(id = test$id, loss = rf.preds)
write_csv(rf.preds.frame, "rf-preds-mat.csv")
```

<<<<<<< HEAD

```{r}
myControl <- trainControl(method = "cv", 
                          number = 3,
                          allowParallel = TRUE)
gbm.grid <- expand.grid("n.trees" = 100, "interaction.depth" = 5, "shrinkage" = c(0.01,0.1), "n.minobsinnode" = 0)

gbm.mod <- train(loss ~.,
                 data = removed_vars,
                 method = "gbm",
                 trControl = myControl,
                 tuneGrid = gbm.grid,
                 metric = "MAE")

gbm.preds <- predict(gbm.mod, test)
gbm.preds.df <- data.frame(id = test$id, loss = gbm.preds)
write_csv(gbm.preds.df, "gbm.preds.csv")

```

```{r}
set.seed(062920)

trainIndex <- createDataPartition(train$loss, p = .05, times = 1, list = FALSE)

trainLoss <- train[trainIndex,]

gbm.part.grid <- expand.grid("n.trees" = 2000, "interaction.depth" = 9, "shrinkage" = .01, "n.minobsinnode" = 0)

gbm.mod.part <- train(loss~.,
                      data = trainLoss,
                      method = "gbm",
                      trControl = myControl,
                      tuneGrid = gbm.part.grid,
                      metric = "MAE")

gbm.part.preds <- predict(gbm.mod.part, test[test$cat89 != "F" & test$cat92 != "E" &
                                               test$cat92 != "G" &
                                               test$cat96 != "H" &
                                               test$cat99 != "U" &
                                               test$cat103 != "M" &
                                               test$cat106 != "Q" &
                                               test$cat109 != "AD" &
                                               test$cat110 != "BH" &
                                               test$cat110 != "CA" &
                                               test$cat110 != "EN" & 
                                               test$cat111 != "L" &
                                               test$cat113 != "R" &
                                               !(test$cat116 %in% c("A", "AI", "AQ", "BE", "BH", "BJ", "BN", "BR", "DB", "EM", "ER", "ET", "EX",
                                                                    "FY", "HS", "IS", "JS", "KO", "LP", "MX", "N")),])
gbm.part.preds.df <- data.frame(id = test$id, loss = gbm.part.preds)
write_csv(gbm.part.preds.df, "gbm.part.preds.csv")

```


=======
```{r}
varImp(rf.model)
```


```{r}
set.seed(062520)
grid <- expand.grid("n.trees" = 434, "interaction.depth" = 7, "shrinkage" = .05, "n.minobsinnode" = c(2, 5, 10, 20, 40))
gbm.model <- train(log(loss)~.,
                   data = removed_vars,
                   method = "gbm",
                   trControl = myControl,
                   tuneGrid = grid,
                   preProc = c("nzv", "zv", "center", "scale"),
                   metric = "MAE")
gbm.model
beepr::beep(sound = 2)
gbm.preds <- predict(gbm.model, test) %>% exp()

# Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : factor cat90 has new levels E, F

gbm.preds.frame <- data.frame(id = test$id, loss = gbm.preds)
write_csv(gbm.preds.frame, "gbm-preds_2.csv")
# 30th percentile is the best this does
```

```{r}
# Need to tune this more. Again 30th percentile
set.seed(062520)
grid <- expand.grid(
  nrounds = 700, 
  max_depth = 11, 
  eta = c(0.01), 
  gamma = 1, 
  colsample_bytree = 0.4, 
  min_child_weight =  1,
  subsample = 1)
xgb.model <- train(log(loss)~.,
                   data = removed_vars,
                   method = "xgbTree",
                   trControl = myControl,
                   tuneGrid = grid,
                   preProc = c("nzv", "zv", "center", "scale"),
                   metric = "MAE")
xgb.model
beepr::beep(sound = 2)
xgb.preds <- predict(xgb.model, test) %>% exp()
xgb.preds.frame <- data.frame(id = test$id, loss = xgb.preds)
write_csv(xgb.preds.frame, "xgb-preds.csv")
```


>>>>>>> 1083e434870fa53182f397d9c85f1b37c01ea97a
