---
title: "R Notebook"
output: pdf_document
---

```{r setup}
library(tidyverse)
library(caret)
library(tidymodels)

train <- read.csv("train.csv")
test <- read.csv("test.csv")
test <- test %>% mutate(loss = NA)
```

```{r}
num_vars <- train %>% select(cont1:cont14, loss)

library(corrplot)
corrplot(cor(num_vars), method = "color", type = "upper", diag = FALSE, addCoef.col = "black", number.digits = 2)

# From the correlation plot, almost none of the continuous variables have a strong correlation with the loss,
# but it looks like there is a good amount of correlation among the continuous variables. Multicolinearity may be a problem with these
```

```{r}
ggplot(train, aes(x = loss)) + geom_histogram(fill = "firebrick4")
train %>%
    filter(loss > 20000) %>%
    summarize(n(), n()/nrow(train))

library(e1071)
skewness(train$loss) # Positive means right skewed, negative means left skewed
# There is a large right skew to the distribution of loss. About .2% of the observations have a loss greater than 20000
box_trans <- BoxCoxTrans(train$loss)
box_trans
# With lambda estimated as 0, the best transformation is the natural log of loss

# Now it is approximately normal with a few outliers
ggplot(train, aes(x = log(loss))) + geom_histogram(fill = "firebrick4")
```

```{r}
myControl <- trainControl(method = "boot",
                          number = 5)

total_prop <- function(data, col){
  data %>% group_by(col) %>% 
    summarize(prop = n()/length(col))
}

total_prop(train, cont1)
train %>% group_by(cat108) %>%
summarize(prop = n()/nrow(train))

# Need to make an X matrix
removed_vars <- train %>% select(-id, -cat89, -cat92, -cat96, -cat99, -cat103, -cat106, -cat109, -cat110, -cat111, -cat113, -cat115, -cat116, -cat114, -cat112)
grid <- expand.grid("alpha" = c(0,.5,1), "lambda" = c(.1, .05,.5, .75, .0001))
glmnet.mod <- train(log(loss)~., 
                    data = removed_vars,
                    method = "glmnet",
                    trControl = myControl,
                    tuneGrid = grid)
glmnet.mod
beepr::beep(sound = 8)
preds <- predict(glmnet.mod, test) %>% exp()
glmnet.preds <- data.frame(id= test$id, loss = preds)
write_csv(glmnet.preds, "glmnet-preds.csv")
```


```{r}
rf.grid <- expand.grid("mtry" = c(20, 40, 80, 160, 200), "splitrule" ="extratrees", "min.node.size" = 1)
rf.model <- train(log(loss)~.,
                  data = removed_vars,
                  method = "ranger",
                  trControl=myControl,
                  tuneGrid = rf.grid)
rf.model
beepr::beep(sound = 2)
```

